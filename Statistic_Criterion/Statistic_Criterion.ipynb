{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook implements the DaNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.regularizers import Regularizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import cv2\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let us begin by loading and preprocessing the source data: MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(source_train_images, source_train_labels), (source_test_images, source_test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_train_images = source_train_images.reshape((60000, 28, 28, 1))\n",
    "source_train_images = source_train_images.reshape((60000, 28*28))\n",
    "source_train_images = source_train_images.astype('float32') / 255\n",
    "\n",
    "#source_test_images = source_test_images.reshape((10000, 28, 28, 1))\n",
    "source_test_images = source_test_images.reshape((10000, 28*28))\n",
    "source_test_images = source_test_images.astype('float32') / 255\n",
    "\n",
    "source_train_labels = to_categorical(source_train_labels)\n",
    "source_test_labels = to_categorical(source_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let us now load and preprocess the target data: USPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('usps_dataset.h5', 'r') as hf:\n",
    "        train = hf.get('train')\n",
    "        target_train_images_aux = train.get('data')[:]\n",
    "        target_train_labels = train.get('target')[:]\n",
    "        test = hf.get('test')\n",
    "        target_test_images_aux = test.get('data')[:]\n",
    "        target_test_labels = test.get('target')[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_images = []\n",
    "target_test_images = []\n",
    "\n",
    "for i in range(7291):\n",
    "    img = target_train_images_aux[i].reshape(16,16)\n",
    "    img = cv2.resize(img, dsize=(28, 28), interpolation=cv2.INTER_CUBIC)\n",
    "    target_train_images.append(img.flatten())\n",
    "    \n",
    "for i in range(2007):\n",
    "    img = target_test_images_aux[i].reshape(16,16)\n",
    "    img = cv2.resize(img, dsize=(28, 28), interpolation=cv2.INTER_CUBIC)\n",
    "    target_test_images.append(img.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert list to numpy arrays\n",
    "target_train_images = np.asarray(target_train_images)\n",
    "target_test_images = np.asarray(target_test_images)\n",
    "\n",
    "#train images\n",
    "#target_train_images = target_train_images.reshape((7291, 28, 28, 1))\n",
    "target_train_images = target_train_images.astype('float32')\n",
    "for i in range(7291):\n",
    "    min_aux = min(target_train_images[i])\n",
    "    max_aux = max(target_train_images[i]-min_aux)\n",
    "    target_train_images[i] = (target_train_images[i]-min_aux)/max_aux\n",
    "\n",
    "#test images\n",
    "#target_test_images = target_test_images.reshape((2007, 28, 28, 1))\n",
    "target_test_images = target_test_images.astype('float32')\n",
    "for i in range(2007):\n",
    "    min_aux = min(target_test_images[i])\n",
    "    max_aux = max(target_test_images[i]-min_aux)\n",
    "    target_test_images[i] = (target_test_images[i]-min_aux)/max_aux\n",
    "\n",
    "#labels\n",
    "target_train_labels = to_categorical(target_train_labels)\n",
    "target_test_labels = to_categorical(target_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "Let us now code the $MMD^2_e(\\boldsymbol{q}_s, \\boldsymbol{\\bar{q}}_t)$ loss where\n",
    "$$\n",
    "\\boldsymbol{q}_s=W_1^Tx_s+b\n",
    "$$ \n",
    "and \n",
    "$$\n",
    "\\boldsymbol{\\bar{q}}_t = W_1^Tx_t+b.\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_distances(x, y):\n",
    "    \n",
    "    if not len(x.get_shape()) == len(y.get_shape()) == 2:\n",
    "        raise ValueError('Both inputs should be matrices.')\n",
    "\n",
    "    if x.get_shape().as_list()[1] != y.get_shape().as_list()[1]:\n",
    "        raise ValueError('The number of features should be the same.')\n",
    "\n",
    "    norm = lambda x: tf.reduce_sum(tf.square(x), 1)\n",
    "    \n",
    "    return tf.transpose(norm(tf.expand_dims(x, 2) - tf.transpose(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_matrix(x, y, sigmas = tf.constant([1e-2, 1e-1, 1, 5, 10])):\n",
    "    \n",
    "    beta = 1. / (2. * (tf.expand_dims(sigmas, 1)))\n",
    "    dist = compute_pairwise_distances(x, y)\n",
    "    s = tf.matmul(beta, tf.reshape(dist, (1, -1)))\n",
    "    \n",
    "    return tf.reshape(tf.reduce_sum(tf.exp(-s), 0), tf.shape(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "def mmd(y_true, y_pred):\n",
    "    \n",
    "    #revert the concatenation so as to recover the source and target outputs of the hidden layer\n",
    "    y_pred_source = y_pred[:, :256]\n",
    "    y_pred_target = y_pred[:, 256:]\n",
    "    \n",
    "    kernel = gaussian_kernel_matrix\n",
    "    cost = tf.reduce_mean(kernel(y_pred_source, y_pred_source))\n",
    "    cost += tf.reduce_mean(kernel(y_pred_target, y_pred_target))\n",
    "    cost -= 2 * tf.reduce_mean(kernel(y_pred_source, y_pred_target))\n",
    "    \n",
    "    #cost has to be non-negative\n",
    "    cost = tf.where(cost > 0, cost, 0)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The model consists of a single hidden layer with 256 nodes.\n",
    "* Inputs: the source and target inputs are paralelly fed into the model (i.e, the model does not change weights).\n",
    "* Outputs: the network has two outputs. The first one is the prediction of the source data and the second one is the concatenation of the outputs of the source and target data by the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vant/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vant/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vant/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_s = layers.Input(shape=(28*28,), name='source_input')\n",
    "input_t = layers.Input(shape=(28*28,), name='target_input')\n",
    "hidden = layers.Dense(256, activation='softplus', name='hidden')\n",
    "#prediction = layers.Dense(10, activation='softmax', name='pred')\n",
    "\n",
    "hidden_s = hidden(input_s)\n",
    "hidden_t = hidden(input_t)\n",
    "aux_output = layers.concatenate([hidden_s, hidden_t], name='aux_output')\n",
    "pred_s = layers.Dense(10, activation='softmax', name='source_output')(hidden_s)\n",
    "#pred_s = prediction(hidden_s)\n",
    "#pred_t = prediction(hidden_t)\n",
    "\n",
    "model = Model([input_s, input_t], [pred_s, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vant/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vant/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-10-a17e87b607d5>:14: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "mmd_weight = 1.\n",
    "model.compile(loss=['categorical_crossentropy', mmd],\n",
    "              loss_weights=[1., mmd_weight], \n",
    "              optimizer='sgd', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All input arrays (x) should have the same number of samples. Got array shapes: [(60000, 784), (7291, 784)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-25419c5afb73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit({'source_input': source_train_images, 'target_input': target_train_images},\n\u001b[1;32m      2\u001b[0m           \u001b[0;34m{\u001b[0m\u001b[0;34m'source_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msource_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aux_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           epochs=5, batch_size=64)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    802\u001b[0m             ]\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    226\u001b[0m         raise ValueError('All input arrays (x) should have '\n\u001b[1;32m    227\u001b[0m                          \u001b[0;34m'the same number of samples. Got array shapes: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                          str([x.shape for x in inputs]))\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         raise ValueError('All target arrays (y) should have '\n",
      "\u001b[0;31mValueError\u001b[0m: All input arrays (x) should have the same number of samples. Got array shapes: [(60000, 784), (7291, 784)]"
     ]
    }
   ],
   "source": [
    "model.fit({'source_input': source_train_images, 'target_input': target_train_images},\n",
    "          {'source_output': source_train_labels, 'aux_output': np.zeros(1)},\n",
    "          epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
